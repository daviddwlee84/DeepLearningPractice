# Model Compression / Knowledge Distillation

## Resources

### Paper

Classic

* [Model compression](https://dl.acm.org/citation.cfm?id=1150464)
* [[1503.02531] Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

Survey

* [FLHonker/Awesome-Knowledge-Distillation: Awesome Knowledge-Distillation.](https://github.com/FLHonker/Awesome-Knowledge-Distillation)
* [[1710.09282] A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282)

### Tools

* [NervanaSystems/distiller: Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.](https://github.com/NervanaSystems/distiller)
  * [Distiller Documentation](https://nervanasystems.github.io/distiller)
* [GMvandeVen/continual-learning: PyTorch implementation of various methods for continual learning (XdG, EWC, online EWC, SI, LwF, DGR, DGR+distill, RtF, iCaRL).](https://github.com/GMvandeVen/continual-learning)

Pytorch

* [peterliht/knowledge-distillation-pytorch: A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility](https://github.com/peterliht/knowledge-distillation-pytorch)
